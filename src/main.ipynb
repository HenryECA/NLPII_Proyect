{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# NLP II: Finetunning Llama\n",
    "\n",
    "This notebook serves as the main pipeline for processing data, creating a model, training it, and evaluating its performance on a text classification task.\n",
    "\n",
    "---\n",
    "## Objectives\n",
    "1. **Data Processing**: Load and preprocess text data.\n",
    "2. **Model Creation**: Define a machine learning or deep learning model for the task.\n",
    "3. **Training**: Train the model on the dataset.\n",
    "4. **Evaluation**: Assess the model's performance on a test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 00:35:54.308337: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-25 00:35:54.314784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732491354.323856   15748 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732491354.326619   15748 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-25 00:35:54.336430: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from time import time\n",
    "\n",
    "from train import train\n",
    "from evaluate import evaluate_model\n",
    "from utils import get_dataset\n",
    "from keys_file import TOKEN\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Data adquisition and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load dataset from ./data/GAIR_lima: Directory ./data/GAIR_lima is neither a `Dataset` directory nor a `DatasetDict` directory.\n",
      "Train size:  99146\n",
      "Test size:  50\n",
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# DATASET = load_dataset(\"GAIR/lima\", data_dir = \"./data\")\n",
    "test_size = 50\n",
    "\n",
    "DATASET = get_dataset(\"FOLDER_DATA\")\n",
    "DATASET['test'] = DATASET['test'].shuffle(seed=42).select(range(min(len(DATASET['test']), test_size)))\n",
    "print(\"Train size: \", len(DATASET[\"train\"]))\n",
    "print(\"Test size: \", len(DATASET[\"test\"]))\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **Configurations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Quantization\n",
    "\n",
    "Quantizing BitsAndBytesConfig reduces memory usage and speeds up inference. The parameters are:\n",
    "\n",
    "* load_in_4bit: Loads the model in 4-bit precision to save memory. (Boolean)\n",
    "\n",
    "* bnb_4bit_quant_type: Sets quantization type (\"nf4\" for accuracy, \"fp4\" for speed).\n",
    "\n",
    "* bnb_4bit_compute_dtype: Defines the computation data type (float16, bfloat16, float32).\n",
    "\n",
    "* bnb_4bit_use_double_quant: Enables double quantization for improved accuracy.\n",
    "\n",
    "**Double quantization**\n",
    "\n",
    "Double quantization reduces quantization error by applying two rounds of quantization.\n",
    "\n",
    "    - The first round for is for the mains weights\n",
    "    - The second round is to capture residual errors, resulting in better model accuracy at a slight cost to speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compute_dtype = getattr(torch, \"bfloat16\")  # Set computation data type to bfloat16 - CHECK\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable loading the model in 4-bit precision\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Specify quantization type as Normal Float 4 - MAYBE INT8 O FLOAT16\n",
    "    bnb_4bit_compute_dtype=compute_dtype, # Set computation data type\n",
    "    bnb_4bit_use_double_quant=True,       # Use double quantization for better accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinmirma/PracticaNLP/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911aa57f879443c9af9e9ea6c298806c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B\"\n",
    "OUTPUT_DIR = \"../models/\" + MODEL_NAME + \"_testing\"\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "\n",
    "# Esto nos prepara el modelo con la config, en la cpu, con la quantizaciÃ³n \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,  # Apply quantization configuration\n",
    "        device_map=\"auto\",                # Automatically map layers to devices\n",
    "        use_auth_token=TOKEN\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': False, '_parameters': OrderedDict(), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_pre_hooks': OrderedDict(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_hooks_with_kwargs': OrderedDict(), '_forward_hooks_always_called': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_forward_pre_hooks_with_kwargs': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_post_hooks': OrderedDict(), '_modules': OrderedDict([('model', LlamaModel(\n",
      "  (embed_tokens): Embedding(128256, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaSdpaAttention(\n",
      "        (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")), ('lm_head', Linear(in_features=4096, out_features=128256, bias=False))]), 'config': LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.1-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      ", 'name_or_path': 'meta-llama/Llama-3.1-8B', 'warnings_issued': {}, 'generation_config': GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      ", '_keep_in_fp32_modules': None, 'vocab_size': 128256, 'is_quantized': True, 'quantization_method': <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>, '_is_hf_initialized': True, '_old_forward': <bound method LlamaForCausalLM.forward of LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")>, '_hf_hook': AlignDevicesHook(execution_device=0, offload=False, io_same_device=True, offload_buffers=False, place_submodules=True, skip_keys=['past_key_values']), 'forward': functools.partial(<function add_hook_to_module.<locals>.new_forward at 0x7f649c2a4b80>, LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")), 'to': <function Module.to at 0x7f6488c36c20>, 'cuda': <function Module.cuda at 0x7f6488c36cb0>, 'hf_device_map': {'': 0}, 'is_loaded_in_4bit': True, 'is_4bit_serializable': True, 'hf_quantizer': <transformers.quantizers.quantizer_bnb_4bit.Bnb4BitHfQuantizer object at 0x7f665c2489d0>}\n"
     ]
    }
   ],
   "source": [
    "print(model.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinmirma/PracticaNLP/.venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    add_eos_token=True,      # Add end-of-sequence token to the tokenizer\n",
    "    use_fast=True,           # Use the fast tokenizer implementation\n",
    "    padding_side='left',      # Pad sequences on the left side,\n",
    "    use_auth_token=TOKEN)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL INSTANTIATION\n",
    "model = prepare_model_for_kbit_training(model) # Por el cuantizado - deja q entrene\n",
    "model.config.pad_token_id = tokenizer.pad_token_id  # Set the model's padding token ID (mirar config del modelo para asegurar nombre)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "Instruction: Give me the intro for a formal email to apply to a Data Scientist position in NVIDIA\n",
      "Context: My name is Patricia and I just finished college\n",
      "Response: Dear Ms. Patricia,\n",
      "I am writing to express my interest in the Data Scientist position at NVIDIA. As a recent graduate with a degree in computer science, I believe my background and skills make me an excellent candidate for this role.\n",
      "I have a strong foundation in data analysis and modeling, having completed coursework in statistics, machine learning, and data visualization. Additionally, I have experience working with large datasets and building predictive models using Python and R. I am confident in my ability to quickly learn new technologies and methodologies, and I am eager to apply my skills to real-world problems in the field of data science.\n",
      "In my previous role as a research assistant at XYZ University, I worked on several projects involving the analysis of large datasets. I have experience with a variety of data analysis tools and techniques, including SQL, Spark, and Hadoop. I am also proficient in the use of statistical software packages such as R and SAS.\n",
      "I am excited about the opportunity to contribute to NVIDIAâs innovative work in the field of data science and I believe that my skills and experience make me an ideal candidate for this position. I would welcome the chance to discuss my qualifications in more detail and to learn more about the work that NVIDIA is doing in this field.\n",
      "Thank you for your consideration. I look forward to hearing from you soon.\n",
      "Dear Ms. Patricia,\n",
      "I am writing to express my interest in the Data Scientist position at NVIDIA. As a recent graduate with a degree in computer science, I believe my background and skills make me an excellent candidate for this role.\n",
      "I have a strong foundation in data analysis and modeling, having completed coursework in statistics, machine learning, and data visualization. Additionally, I have experience working with large datasets and building predictive models using Python and R. I am confident in my ability to quickly learn new technologies and methodologies, and I am eager to apply my skills to real-world problems in the field of data science.\n",
      "In my previous role as a research assistant at XYZ University, I worked on several projects involving the analysis of large datasets. I have experience with a variety of data analysis tools and techniques, including SQL, Spark, and Hadoop. I am also proficient in the use of statistical software packages such as R and SAS.\n",
      "I am excited about the opportunity to contribute to NVIDIAâs innovative work in the field of data science\n"
     ]
    }
   ],
   "source": [
    "# testing model before training\n",
    "def generate_text(model, tokenizer, prompt, device=\"cuda\"):\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(inputs['input_ids'], max_length=500, num_return_sequences=1)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# test_prompt = \"Instruction: Give me a formal email intro\\nContext: I am a law student applying to a New York like the Suits TV show\\nResponse: \"\n",
    "test_prompt = \"Instruction: Give me the intro for a formal email to apply to a Data Scientist position in NVIDIA\\nContext: My name is Patricia and I just finished college\\nResponse:\"\n",
    "\n",
    "try: \n",
    "    # Get model output before training\n",
    "    print(\"Before training:\")\n",
    "    output_before = generate_text(model, tokenizer, test_prompt, DEVICE)\n",
    "    print(output_before)\n",
    "except:\n",
    "      pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 3. Finetuning Parameters\n",
    "\n",
    "Decide which of the techniques we want to implement in each run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 - LoRA\n",
    "\n",
    "Applies low-rank updates to pretrained models, enabling efficient fine-tuning by learning only small, additional matrices instead of updating all model weights. Hereâs what each parameter does:\n",
    "\n",
    "* lora_alpha: Scaling factor for updates; higher values (16, 32) increase update impact, improving adaptation but may risk overfitting.\n",
    "\n",
    "* lora_dropout: Dropout rate for LoRA layers; typical values (0.0, 0.05) help prevent overfitting with minimal regularization.\n",
    "\n",
    "* r: Rank of LoRA matrices; lower values (4, 8) reduce parameters and memory, while higher values (16) offer more flexibility.\n",
    "\n",
    "* bias: Adds bias term (\"none\", \"all\", \"lora_only\") to control if and where bias adjustments are made.\n",
    "\n",
    "* target_modules: Specifies layers to apply LoRA (['k_proj', 'v_proj']); selecting fewer layers reduces compute cost but may limit effectiveness.\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNotes on how to improve:\\nAfter fine-tuning, check the validation loss. If it\\'s high, try making the following adjustments one at a time:\\nIncrease lora_alpha: If the model is underfitting, try increasing lora_alpha to 32 or 64.\\nIncrease lora_dropout: If you observe overfitting, increase lora_dropout to 0.2 or 0.3.\\nDecrease r: If the model is too large or overfitting, reduce r to 8 or 4.\\nReduce the number of target modules: If the model is overfitting, try applying LoRA to fewer modules, such as [\\'q_proj\\', \\'v_proj\\'] or just [\\'k_proj\\', \\'o_proj\\'].\\n\\n\\n\\nTrial 1: vAL lOSS : 2.88 - 2.74 - 2.68 - 2.65\\nlora_alpha=16,            \\nlora_dropout=0.05,          \\nr=16,                      \\nbias=\"none\",               \\ntask_type=\"CAUSAL_LM\",     \\ntarget_modules=[\\'k_proj\\', \\'q_proj\\', \\'v_proj\\', \\'o_proj\\',\\'gate_proj\\', \\'down_proj\\', \\'up_proj\\']\\n\\nTrial 2: vAL lOSS : 2.88 - 2.75 - No more\\nlora_alpha=16,            \\nlora_dropout=0.2,          \\nr=16,                      \\nbias=\"none\",               \\ntask_type=\"CAUSAL_LM\",     \\ntarget_modules=[\\'k_proj\\', \\'q_proj\\', \\'v_proj\\', \\'o_proj\\',\\'gate_proj\\', \\'down_proj\\', \\'up_proj\\']\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "            lora_alpha=16,             # Scaling factor for LoRA updates\n",
    "            lora_dropout=0.15,          # Dropout rate applied to LoRA layers\n",
    "            r=8,                      # Rank of the LoRA decomposition\n",
    "            bias=\"none\",               # No bias is added to the LoRA layers\n",
    "            task_type=\"CAUSAL_LM\",     # Specify the task as causal language modeling\n",
    "            target_modules=[           # Modules to apply LoRA to\n",
    "                'k_proj', 'q_proj', 'v_proj', 'o_proj',\n",
    "                'gate_proj', 'down_proj', 'up_proj'\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Notes on how to improve:\n",
    "After fine-tuning, check the validation loss. If it's high, try making the following adjustments one at a time:\n",
    "Increase lora_alpha: If the model is underfitting, try increasing lora_alpha to 32 or 64.\n",
    "Increase lora_dropout: If you observe overfitting, increase lora_dropout to 0.2 or 0.3.\n",
    "Decrease r: If the model is too large or overfitting, reduce r to 8 or 4.\n",
    "Reduce the number of target modules: If the model is overfitting, try applying LoRA to fewer modules, such as ['q_proj', 'v_proj'] or just ['k_proj', 'o_proj'].\n",
    "\n",
    "\n",
    "\n",
    "Trial 1: vAL lOSS : 2.88 - 2.74 - 2.68 - 2.65\n",
    "lora_alpha=16,            \n",
    "lora_dropout=0.05,          \n",
    "r=16,                      \n",
    "bias=\"none\",               \n",
    "task_type=\"CAUSAL_LM\",     \n",
    "target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj','gate_proj', 'down_proj', 'up_proj']\n",
    "\n",
    "Trial 2: vAL lOSS : 2.88 - 2.75 - No more\n",
    "lora_alpha=16,            \n",
    "lora_dropout=0.2,          \n",
    "r=16,                      \n",
    "bias=\"none\",               \n",
    "task_type=\"CAUSAL_LM\",     \n",
    "target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj','gate_proj', 'down_proj', 'up_proj']\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 - AdaLora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loha config\n",
    "from peft import AdaLoraConfig\n",
    "\n",
    "adalora_config = AdaLoraConfig(\n",
    "        peft_type=\"ADALORA\", \n",
    "        task_type=\"CAUSAL_LM\", \n",
    "        init_r=8, lora_alpha=16, \n",
    "        target_modules=[           # Modules to apply LoRA to\n",
    "                'k_proj', 'q_proj', 'v_proj', 'o_proj',\n",
    "                'gate_proj', 'down_proj', 'up_proj'\n",
    "            ],\n",
    "        lora_dropout=0.15,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 - VbLora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import VBLoRAConfig\n",
    "\n",
    "vb_config = VBLoRAConfig(\n",
    "            num_vectors=2048,          # Dropout rate applied to VeRA layers\n",
    "            vector_length=256,\n",
    "            r=4,                      # Rank of the LoRA decomposition\n",
    "            topk=2, \n",
    "            bias=\"none\",               # No bias is added to the VeRA layers\n",
    "            target_modules=[           # Modules to apply LoRA to\n",
    "                'k_proj', 'q_proj', 'v_proj', 'o_proj',\n",
    "            ]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 - Llama Adapter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AdaptionPromptConfig\n",
    "\n",
    "llama_adapter = AdaptionPromptConfig(\n",
    "    adapter_len=20,\n",
    "    adapter_layers=16,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[           # Modules to apply LoRA to\n",
    "                'k_proj', 'q_proj', 'v_proj', 'o_proj',\n",
    "                'gate_proj', 'down_proj', 'up_proj'\n",
    "            ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LLama-Adapter (10 len, 30 layers, no lora)\n",
    "    The model seems worse than LoRA. It achieves a training loss of 1.8 and a val loss of 1.61\n",
    "\n",
    "- Llama-Adapter (16 len, 16 layers, lora)\n",
    "    Model is worse than only lora. It achieves training loss of 1.72 and val loss of 1.76"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 5. Training Parameters\n",
    "\n",
    "* output_dir: Directory to save checkpoints and logs.\n",
    "* eval_strategy: When to run evaluation (\"steps\" or \"epoch\").\n",
    "* do_eval: Enable/disable evaluation during training.\n",
    "* optim: Optimizer type (\"paged_adamw_8bit\" for memory-efficient AdamW).\n",
    "* per_device_train_batch_size: Batch size per device for training.\n",
    "* gradient_accumulation_steps: Accumulate gradients over steps for larger effective batch size.\n",
    "* per_device_eval_batch_size: Batch size per device for evaluation.\n",
    "* log_level: Logging verbosity level (\"debug\" for detailed logs).\n",
    "* logging_steps: Log metrics every N steps.\n",
    "* learning_rate: Initial learning rate for optimization.\n",
    "* eval_steps: Run evaluation every N steps.\n",
    "* max_steps: Total number of training steps.\n",
    "* save_steps: Save model checkpoints every N steps.\n",
    "* warmup_steps: Steps to gradually increase learning rate.\n",
    "* lr_scheduler_type: Type of learning rate scheduler (\"linear\" for steady decay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunciÃ³n unificada para configurar TrainingArguments\n",
    "def create_training_args(output_dir, learning_rate, batch_size, num_epochs=3, additional_args=None):\n",
    "    additional_args = additional_args or {}\n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"steps\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        gradient_accumulation_steps=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        logging_steps=10,\n",
    "        eval_steps=25,\n",
    "        max_steps=100,\n",
    "        save_steps=25,\n",
    "        warmup_steps=25,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        **additional_args,  # Permite agregar argumentos adicionales segÃºn sea necesario\n",
    "    )\n",
    "\n",
    "def objective(trial):\n",
    "    # Define el espacio de bÃºsqueda de hiperparÃ¡metros\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [2, 4, 8, 16])\n",
    "\n",
    "    # Configura el modelo con los hiperparÃ¡metros sugeridos\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        num_train_epochs=3,  # Puedes ajustarlo segÃºn el caso\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        optim=\"paged_adamw_8bit\", \n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=DATASET[\"train\"],\n",
    "        eval_dataset=DATASET[\"test\"],\n",
    "        dataset_text_field=\"prompt\", \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # Entrena el modelo y obtÃ©n la mÃ©trica de evaluaciÃ³n\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    return eval_results[\"eval_loss\"]\n",
    "\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(objective, n_trials=1)\n",
    "\n",
    "\n",
    "# # Actualiza el modelo con los mejores hiperparÃ¡metros\n",
    "# best_params = study.best_trial.params\n",
    "# LEARNING_RATE = best_params[\"learning_rate\"]\n",
    "# PER_DEVICE_TRAIN_BATCH_SIZE = best_params[\"per_device_train_batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Imprime el mejor resultado\n",
    "# print(\"Best trial:\")\n",
    "# print(f\"  Loss: {study.best_trial.value}\")\n",
    "# print(\"  Hyperparameters:\")\n",
    "# for key, value in study.best_trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")\n",
    "\n",
    "# # Entrenar el modelo con los mejores hiperparÃ¡metros\n",
    "# best_params = study.best_trial.params\n",
    "# training_arguments = create_training_args(\n",
    "#     output_dir=\"./results_best\",\n",
    "#     learning_rate=best_params[\"learning_rate\"],\n",
    "#     batch_size=best_params[\"per_device_train_batch_size\"],\n",
    "#     num_epochs=5,  # Mayor nÃºmero de Ã©pocas para el modelo final\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00018335806063256405\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,  # Directory for saving model checkpoints and logs\n",
    "    eval_strategy=\"steps\",                # Evaluation strategy: evaluate every few steps\n",
    "    do_eval=True,                         # Enable evaluation during training\n",
    "    optim=\"paged_adamw_8bit\",             # Use 8-bit AdamW optimizer for memory efficiency\n",
    "    per_device_train_batch_size=BATCH_SIZE,        # Batch size per device during training\n",
    "    gradient_accumulation_steps=2,        # Accumulate gradients over multiple steps\n",
    "    per_device_eval_batch_size=BATCH_SIZE,         # Batch size per device during evaluation\n",
    "    log_level=\"debug\",                    # Set logging level to debug for detailed logs\n",
    "    logging_steps=10,                     # Log metrics every 10 steps\n",
    "    learning_rate=LEARNING_RATE,          # Initial learning rate\n",
    "    eval_steps=200,                        # Evaluate the model every 25 steps\n",
    "    max_steps=50000,                        # Total number of training steps\n",
    "    save_steps=250,                        # Save checkpoints every 25 steps\n",
    "    warmup_steps=250,                      # Number of warmup steps for learning rate scheduler\n",
    "    lr_scheduler_type=\"linear\",           # Use a linear learning rate scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impelement teh different fine tuning configurations\n",
    "# Booleans to manage finetuning techniques implementation\n",
    "\n",
    "implement_lora = True\n",
    "implement_adalora = False\n",
    "implement_vb = False\n",
    "implement_llama_adapter = True\n",
    "\n",
    "\n",
    "if implement_llama_adapter:\n",
    "        model = get_peft_model(model, llama_adapter)\n",
    "        # model.add_adapter(\"llama-adapter\", llama_adapter)\n",
    "        \n",
    "if implement_lora:\n",
    "        lora_config = lora_config\n",
    "        model = get_peft_model(model, lora_config)\n",
    "\n",
    "if implement_adalora:\n",
    "        model = get_peft_model(model, adalora_config)\n",
    "\n",
    "if implement_vb:\n",
    "        model = get_peft_model(model, vb_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinmirma/PracticaNLP/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/joaquinmirma/PracticaNLP/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/joaquinmirma/PracticaNLP/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd8339cd93549b9949179b6ea3e0a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99146 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cc7bc20a7f4053972661f55f0c4434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquinmirma/PracticaNLP/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Currently training with a batch size of: 2\n",
      "***** Running training *****\n",
      "  Num examples = 99,146\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 50,000\n",
      "  Number of trainable parameters = 1,310,736\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/joaquinmirma/PracticaNLP/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/joaquinmirma/PracticaNLP/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='50000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   63/50000 04:01 < 54:49:32, 0.25 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model with the specified training arguments\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_arguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATASET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PracticaNLP/NLPII_Proyect/src/train.py:47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, tokenizer, training_arguments, tokenized_dataset, device, output_dir)\u001b[0m\n\u001b[1;32m     35\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     36\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                          \u001b[38;5;66;03m# The pre-trained and prepared model\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m tokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Training dataset\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Start the fine-tuning process\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Save the trained model and tokenizer\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_dir):\n",
      "File \u001b[0;32m~/PracticaNLP/.venv/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PracticaNLP/.venv/lib/python3.10/site-packages/transformers/trainer.py:2486\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model with the specified training arguments\n",
    "model = train(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    training_arguments=training_arguments,\n",
    "\n",
    "    tokenized_dataset=DATASET,\n",
    "    device=DEVICE,\n",
    "    output_dir=OUTPUT_DIR,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.path.exists(OUTPUT_DIR))  # Should return True if the path is valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['src', 'starter_kit', 'NLP2 - Final_Project.pdf', '.gitignore', 'README.md', 'IfEval', '.git', 'models']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 12:15:08.657230: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-24 12:15:08.663662: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732446908.679745   12472 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732446908.682164   12472 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-24 12:15:08.691088: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea08e80b47494ce695703844edbb3326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the model and tokenizer\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "print(os.listdir(\"../\"))\n",
    "OUTPUT_DIR = \"../models/meta-llama/Llama-3.1-8B_testing/checkpoint-14000\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"bfloat16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=OUTPUT_DIR,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "# tokenizer.to(DEVICE)\n",
    "\n",
    "# Evaluate\n",
    "# evaluate_model(model, tokenizer, DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training:\n",
      "Instruction: Give me the intro for a formal email to apply to a Data Scientist position in NVIDIA\n",
      "Context: My name is Patricia and I just finished college\n",
      "Response: Dear Hiring Manager,\n",
      "\n",
      "My name is Patricia and I am excited to apply for the Data Scientist position at NVIDIA. As a recent graduate, I am eager to put my skills and knowledge to work in a dynamic and innovative company like NVIDIA.\n",
      "\n",
      "I have a strong background in mathematics, statistics, and machine learning, which has equipped me with the necessary skills to excel in this role. I am also proficient in programming languages such as Python and R, and have experience working with large datasets using tools such as SQL and Apache Spark.\n",
      "\n",
      "In addition to my technical skills, I am a strong communicator and collaborator. I am able to effectively communicate complex technical concepts to non-technical audiences, and have experience working in cross-functional teams to achieve common goals.\n",
      "\n",
      "Thank you for considering my application. I am looking forward to the opportunity to contribute to the success of NVIDIA and its mission to advance the state of the art in artificial intelligence and computing.\n",
      "\n",
      "Best regards,\n",
      "Patricia. \n",
      "\n",
      "I hope this helps! Please let me know if you have any other questions. I'd be happy to help. ðððð. Is there anything else I can help you with? ð¤ð¤ð¤. Let me know how I can help you. ðððð. I'd be happy to assist you with anything. ðððð. Let me know if you have any other questions. I'd be happy to answer them. ðððð. I'd be happy to assist you with anything. ðððð. Let me know if you have any other questions. I'd be happy to answer them. ðððð. I'd be happy to assist you with anything. ðððð. Let me know if you have any other questions. I'd be happy to answer them. ðððð. I'd be happy to assist you with anything. ðððð. Let me know if you have any other questions. I'd be happy to answer them. ï¿½\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, device=\"cuda\"):\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(inputs['input_ids'], max_length=500, num_return_sequences=1)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# test_prompt = \"Instruction: Give me a formal email intro\\nContext: I am a law student applying to a New York like the Suits TV show\\nResponse: \"\n",
    "test_prompt = \"Instruction: Give me the intro for a formal email to apply to a Data Scientist position in NVIDIA\\nContext: My name is Patricia and I just finished college\\nResponse:\"\n",
    "\n",
    "\n",
    "\n",
    "# Checking trained model performance\n",
    "print(\"After training:\")\n",
    "output_after = generate_text(model, tokenizer, test_prompt, DEVICE)\n",
    "print(output_after)\n",
    "\n",
    "\n",
    "# Make sure the model is retrieved or saved after training!!!!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
