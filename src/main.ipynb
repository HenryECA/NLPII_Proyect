{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# NLP II: Finetunning Llama\n",
    "\n",
    "This notebook serves as the main pipeline for processing data, creating a model, training it, and evaluating its performance on a text classification task.\n",
    "\n",
    "---\n",
    "## Objectives\n",
    "1. **Data Processing**: Load and preprocess text data.\n",
    "2. **Model Creation**: Define a machine learning or deep learning model for the task.\n",
    "3. **Training**: Train the model on the dataset.\n",
    "4. **Evaluation**: Assess the model's performance on a test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 12:35:43.970050: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-21 12:35:44.175881: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-21 12:35:44.252258: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-21 12:35:44.274057: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-21 12:35:44.422967: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-21 12:35:45.398914: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from time import time\n",
    "\n",
    "from train import train\n",
    "from evaluate import evaluate_model\n",
    "from utils import get_dataset\n",
    "from keys_file import TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Data adquisition and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  1000\n",
      "Test size:  50\n",
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# DATASET = load_dataset(\"GAIR/lima\", data_dir = \"./data\")\n",
    "test_size = 50\n",
    "\n",
    "DATASET = get_dataset(\"FOLDER_DATA\")\n",
    "DATASET['train'] = DATASET['train'].shuffle(seed=42).select(range(min(len(DATASET['test']), 1000)))\n",
    "DATASET['test'] = DATASET['test'].shuffle(seed=42).select(range(min(len(DATASET['test']), test_size)))\n",
    "print(\"Train size: \", len(DATASET[\"train\"]))\n",
    "print(\"Test size: \", len(DATASET[\"test\"]))\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **Configurations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Quantization\n",
    "\n",
    "Quantizing BitsAndBytesConfig reduces memory usage and speeds up inference. The parameters are:\n",
    "\n",
    "* load_in_4bit: Loads the model in 4-bit precision to save memory. (Boolean)\n",
    "\n",
    "* bnb_4bit_quant_type: Sets quantization type (\"nf4\" for accuracy, \"fp4\" for speed).\n",
    "\n",
    "* bnb_4bit_compute_dtype: Defines the computation data type (float16, bfloat16, float32).\n",
    "\n",
    "* bnb_4bit_use_double_quant: Enables double quantization for improved accuracy.\n",
    "\n",
    "**Double quantization**\n",
    "\n",
    "Double quantization reduces quantization error by applying two rounds of quantization.\n",
    "\n",
    "    - The first round for is for the mains weights\n",
    "    - The second round is to capture residual errors, resulting in better model accuracy at a slight cost to speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compute_dtype = getattr(torch, \"bfloat16\")  # Set computation data type to bfloat16 - CHECK\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable loading the model in 4-bit precision\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Specify quantization type as Normal Float 4 - MAYBE INT8 O FLOAT16\n",
    "    bnb_4bit_compute_dtype=compute_dtype, # Set computation data type\n",
    "    bnb_4bit_use_double_quant=True,       # Use double quantization for better accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usuario/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d5477fbce745b7ac76f32b387e474a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B\"\n",
    "OUTPUT_DIR = \"./\" + MODEL_NAME + \"_results\"\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "\n",
    "# Esto nos prepara el modelo con la config, en la cpu, con la quantización \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,  # Apply quantization configuration\n",
    "        device_map=\"auto\",                # Automatically map layers to devices\n",
    "        use_auth_token=TOKEN\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usuario/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    add_eos_token=True,      # Add end-of-sequence token to the tokenizer\n",
    "    use_fast=True,           # Use the fast tokenizer implementation\n",
    "    padding_side='left',      # Pad sequences on the left side,\n",
    "    use_auth_token=TOKEN)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL INSTANTIATION\n",
    "model = prepare_model_for_kbit_training(model) # Por el cuantizado - deja q entrene\n",
    "model.config.pad_token_id = tokenizer.pad_token_id  # Set the model's padding token ID (mirar config del modelo para asegurar nombre)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "Instruction: Dame una receta de gazpacho andaluz\n",
      "Context: Soy un inutil\n",
      "Response: Gazpacho Andaluz:\n",
      "\n",
      "Ingredientes:\n",
      "\n",
      "- 1 taza de tomates maduros y jugosos\n",
      "- 1 taza de pepinos en rodajas\n",
      "- 1 cebolla mediana picada\n",
      "- 1 pimiento rojo en rodajas\n",
      "- 1 pimiento verde en rodajas\n",
      "- 1 diente de ajo\n"
     ]
    }
   ],
   "source": [
    "# testing model before training\n",
    "def generate_text(model, tokenizer, prompt, device=\"cuda\"):\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(inputs['input_ids'], max_length=500, num_return_sequences=1)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# test_prompt = \"Instruction: Give me a formal email intro\\nContext: I am a law student applying to a New York like suits\\nResponse: \"\n",
    "test_prompt = \"Instruction: Dame una receta de gazpacho andaluz\\nContext: Soy un inutil\\nResponse:\"\n",
    "\n",
    "try: \n",
    "    # Get model output before training\n",
    "    print(\"Before training:\")\n",
    "    output_before = generate_text(model, tokenizer, test_prompt, DEVICE)\n",
    "    print(output_before)\n",
    "except:\n",
    "      pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Training Parameters\n",
    "\n",
    "* output_dir: Directory to save checkpoints and logs.\n",
    "* eval_strategy: When to run evaluation (\"steps\" or \"epoch\").\n",
    "* do_eval: Enable/disable evaluation during training.\n",
    "* optim: Optimizer type (\"paged_adamw_8bit\" for memory-efficient AdamW).\n",
    "* per_device_train_batch_size: Batch size per device for training.\n",
    "* gradient_accumulation_steps: Accumulate gradients over steps for larger effective batch size.\n",
    "* per_device_eval_batch_size: Batch size per device for evaluation.\n",
    "* log_level: Logging verbosity level (\"debug\" for detailed logs).\n",
    "* logging_steps: Log metrics every N steps.\n",
    "* learning_rate: Initial learning rate for optimization.\n",
    "* eval_steps: Run evaluation every N steps.\n",
    "* max_steps: Total number of training steps.\n",
    "* save_steps: Save model checkpoints every N steps.\n",
    "* warmup_steps: Steps to gradually increase learning rate.\n",
    "* lr_scheduler_type: Type of learning rate scheduler (\"linear\" for steady decay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,  # Directory for saving model checkpoints and logs\n",
    "    eval_strategy=\"steps\",                # Evaluation strategy: evaluate every few steps\n",
    "    do_eval=True,                         # Enable evaluation during training\n",
    "    optim=\"paged_adamw_8bit\",             # Use 8-bit AdamW optimizer for memory efficiency\n",
    "    per_device_train_batch_size=4,        # Batch size per device during training\n",
    "    gradient_accumulation_steps=2,        # Accumulate gradients over multiple steps\n",
    "    per_device_eval_batch_size=2,         # Batch size per device during evaluation\n",
    "    log_level=\"debug\",                    # Set logging level to debug for detailed logs\n",
    "    logging_steps=10,                     # Log metrics every 10 steps\n",
    "    learning_rate=LEARNING_RATE,                   # Initial learning rate\n",
    "    eval_steps=25,                        # Evaluate the model every 25 steps\n",
    "    max_steps=100,                        # Total number of training steps\n",
    "    save_steps=25,                        # Save checkpoints every 25 steps\n",
    "    warmup_steps=25,                      # Number of warmup steps for learning rate scheduler\n",
    "    lr_scheduler_type=\"linear\",           # Use a linear learning rate scheduler\n",
    ")\n",
    "\n",
    "# gradient pero tira de memoria\n",
    "# mas epochs menos tocar training params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Finetuning Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 - LoRA\n",
    "\n",
    "Applies low-rank updates to pretrained models, enabling efficient fine-tuning by learning only small, additional matrices instead of updating all model weights. Here’s what each parameter does:\n",
    "\n",
    "* lora_alpha: Scaling factor for updates; higher values (16, 32) increase update impact, improving adaptation but may risk overfitting.\n",
    "\n",
    "* lora_dropout: Dropout rate for LoRA layers; typical values (0.0, 0.05) help prevent overfitting with minimal regularization.\n",
    "\n",
    "* r: Rank of LoRA matrices; lower values (4, 8) reduce parameters and memory, while higher values (16) offer more flexibility.\n",
    "\n",
    "* bias: Adds bias term (\"none\", \"all\", \"lora_only\") to control if and where bias adjustments are made.\n",
    "\n",
    "* target_modules: Specifies layers to apply LoRA (['k_proj', 'v_proj']); selecting fewer layers reduces compute cost but may limit effectiveness.\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNotes on how to improve:\\nAfter fine-tuning, check the validation loss. If it\\'s high, try making the following adjustments one at a time:\\nIncrease lora_alpha: If the model is underfitting, try increasing lora_alpha to 32 or 64.\\nIncrease lora_dropout: If you observe overfitting, increase lora_dropout to 0.2 or 0.3.\\nDecrease r: If the model is too large or overfitting, reduce r to 8 or 4.\\nReduce the number of target modules: If the model is overfitting, try applying LoRA to fewer modules, such as [\\'q_proj\\', \\'v_proj\\'] or just [\\'k_proj\\', \\'o_proj\\'].\\n\\n\\n\\nTrial 1: vAL lOSS : 2.88 - 2.74 - 2.68 - 2.65\\nlora_alpha=16,            \\nlora_dropout=0.05,          \\nr=16,                      \\nbias=\"none\",               \\ntask_type=\"CAUSAL_LM\",     \\ntarget_modules=[\\'k_proj\\', \\'q_proj\\', \\'v_proj\\', \\'o_proj\\',\\'gate_proj\\', \\'down_proj\\', \\'up_proj\\']\\n\\nTrial 2: vAL lOSS : 2.88 - 2.75 - No more\\nlora_alpha=16,            \\nlora_dropout=0.2,          \\nr=16,                      \\nbias=\"none\",               \\ntask_type=\"CAUSAL_LM\",     \\ntarget_modules=[\\'k_proj\\', \\'q_proj\\', \\'v_proj\\', \\'o_proj\\',\\'gate_proj\\', \\'down_proj\\', \\'up_proj\\']\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "            lora_alpha=16,             # Scaling factor for LoRA updates\n",
    "            lora_dropout=0.15,          # Dropout rate applied to LoRA layers\n",
    "            r=8,                      # Rank of the LoRA decomposition\n",
    "            bias=\"none\",               # No bias is added to the LoRA layers\n",
    "            task_type=\"CAUSAL_LM\",     # Specify the task as causal language modeling\n",
    "            target_modules=[           # Modules to apply LoRA to\n",
    "                'k_proj', 'q_proj', 'v_proj', 'o_proj',\n",
    "                'gate_proj', 'down_proj', 'up_proj'\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Notes on how to improve:\n",
    "After fine-tuning, check the validation loss. If it's high, try making the following adjustments one at a time:\n",
    "Increase lora_alpha: If the model is underfitting, try increasing lora_alpha to 32 or 64.\n",
    "Increase lora_dropout: If you observe overfitting, increase lora_dropout to 0.2 or 0.3.\n",
    "Decrease r: If the model is too large or overfitting, reduce r to 8 or 4.\n",
    "Reduce the number of target modules: If the model is overfitting, try applying LoRA to fewer modules, such as ['q_proj', 'v_proj'] or just ['k_proj', 'o_proj'].\n",
    "\n",
    "\n",
    "\n",
    "Trial 1: vAL lOSS : 2.88 - 2.74 - 2.68 - 2.65\n",
    "lora_alpha=16,            \n",
    "lora_dropout=0.05,          \n",
    "r=16,                      \n",
    "bias=\"none\",               \n",
    "task_type=\"CAUSAL_LM\",     \n",
    "target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj','gate_proj', 'down_proj', 'up_proj']\n",
    "\n",
    "Trial 2: vAL lOSS : 2.88 - 2.75 - No more\n",
    "lora_alpha=16,            \n",
    "lora_dropout=0.2,          \n",
    "r=16,                      \n",
    "bias=\"none\",               \n",
    "task_type=\"CAUSAL_LM\",     \n",
    "target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj','gate_proj', 'down_proj', 'up_proj']\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment to avoid LoRA\n",
    "# lora_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 - PEFT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config= \"\"\n",
    "\n",
    "# Uncomment to avoid using PEFT\n",
    "peft_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 - Supervised Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = \"\"\n",
    "\n",
    "# Uncomment to avoid SFT\n",
    "sft_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 - To be determined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usuario/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/usuario/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/usuario/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d222b55b9678483c8b1a391d30b64408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c61202e4e34c5ab2f044f9fae44d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usuario/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Currently training with a batch size of: 4\n",
      "***** Running training *****\n",
      "  Num examples = 1,000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 20,971,520\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/usuario/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/100 : < :, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model with the specified training arguments\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_arguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATASET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Make sure the model is retrieved or saved after training!!!!!!\u001b[39;00m\n",
      "File \u001b[0;32m~/FinalProyectNLPII/NLPII_Proyect/src/train.py:57\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, tokenizer, training_arguments, tokenized_dataset, device, output_dir, lora_config, peft_config, sft_config)\u001b[0m\n\u001b[1;32m     45\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     46\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                          \u001b[38;5;66;03m# The pre-trained and prepared model\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m tokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Training dataset\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Start the fine-tuning process\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Save the trained model and tokenizer\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_dir):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:434\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 434\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2393\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model with the specified training arguments\n",
    "model = train(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    training_arguments=training_arguments,\n",
    "\n",
    "    tokenized_dataset=DATASET,\n",
    "    device=DEVICE,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "\n",
    "    lora_config=lora_config,\n",
    "    peft_config=None,\n",
    "    sft_config=None,\n",
    ")\n",
    "\n",
    "# Checking trained model performance\n",
    "print(\"After training:\")\n",
    "output_after = generate_text(model, tokenizer, test_prompt, DEVICE)\n",
    "print(output_after)\n",
    "\n",
    "\n",
    "# Make sure the model is retrieved or saved after training!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/usuario/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.1-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/usuario/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/model.safetensors.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38b2128bb6745af9effe12229e9f1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.1-8B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/usuario/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "loading file tokenizer.json\n",
      "loading file tokenizer.model\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training:\n",
      "Instruction: Give me a formal email intro\n",
      "Context: I am a law student applying to a New York like suits\n",
      "Response:  Dear [Name of the recipient],\n",
      "\n",
      "I am writing to express my interest in the [Position] position that is currently open at [Company]. I am a recent graduate of [University] with a degree in [Major] and I am currently pursuing a law degree. I believe that my education, skills, and experience make me an excellent candidate for this position.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR)\n",
    "model.to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "# tokenizer.to(DEVICE)\n",
    "\n",
    "# Evaluate\n",
    "# evaluate_model(model, tokenizer, DATASET)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
